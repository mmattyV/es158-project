# Project Proposal: Multi-Agent Reinforcement Learning for Real-Time Frequency Regulation in Power Grids with Renewable Integration

## 1. Relevance to the Course (Sequential Decision Making in Dynamic Environments)

This project addresses distributed optimal control in power system frequency regulation, formulated as a **Multi-Agent Markov Decision Process (MA-MDP)** where:

- **Decision-makers**: Multiple controllable generation units (batteries, natural gas peakers, demand response aggregators) that must coordinate to maintain grid frequency at 60 Hz
- **Dynamics**: Grid frequency evolves according to swing equations‚Äîdetermined by power balance between generation and load. Each agent's action affects system-wide frequency through coupled electromechanical dynamics:
  ```
  df/dt = (P_gen - P_load - P_losses) / (2H¬∑S_base)
  ```
  where H is system inertia, and each agent contributes to total P_gen
- **Sequential nature**: Control decisions must anticipate renewable generation forecasts (solar/wind ramps), load fluctuations, and other agents' actions. Incorrect responses cause cascading frequency deviations that take minutes to correct, requiring multi-step lookahead planning

The problem exhibits core RL characteristics: continuous high-dimensional state/action spaces, partial observability (agents see local measurements with communication delays), stochastic disturbances (renewable intermittency, unexpected load changes), and critical safety constraints (frequency must stay within ¬±0.5 Hz to prevent equipment damage and blackouts). The **multi-agent coordination** aspect introduces challenges beyond single-agent RL: credit assignment across agents, non-stationarity from simultaneously learning policies, and scalability to realistic grid sizes (50+ agents).

---

## 2. Motivation and Related Work

### Why This Matters

The rapid integration of renewable energy fundamentally disrupts power grid operations. Solar and wind now comprise >30% of generation in many regions [EIA, 2024], but their intermittency creates severe frequency control challenges. Traditional synchronous generators provide natural inertia that stabilizes frequency; renewables (inverter-based) do not. This "low-inertia" problem causes:
- **Faster frequency dynamics**: Rate of change of frequency (RoCoF) has doubled in some grids [NERC, 2023]
- **Increased regulation costs**: Grid operators pay $10B+ annually for frequency services in the U.S. alone
- **Reliability risks**: Several major blackouts (South Australia 2016, Texas 2021) linked to inadequate frequency response

Current solutions rely on costly fast-response reserves or curtail renewable generation. **Data-driven multi-agent RL** offers coordinated, adaptive control that learns optimal dispatch strategies from operational data, potentially reducing costs by 20-40% while improving reliability [Venkat et al., 2022].

### Challenge

The core technical barriers:

1. **Safety-critical real-time control**: Frequency deviations >1 Hz trigger automatic load shedding (blackouts). Controllers must guarantee constraint satisfaction, not just optimize in expectation.

2. **Scalability**: Realistic transmission systems have 100+ controllable units. Centralized control suffers from communication bottlenecks and single points of failure; fully decentralized control leads to suboptimal coordination.

3. **Partial observability and delays**: Agents observe local bus frequency/voltage with 2-4 second telemetry delays. True system state (all bus frequencies, line flows) is not directly observable.

4. **Non-stationary environment**: Load patterns shift hourly/seasonally, renewable output depends on weather, and topology changes due to line/generator outages. Policies must adapt online without catastrophic forgetting.

5. **Multi-objective tradeoffs**: Minimize regulation cost AND frequency deviation AND wear on equipment (battery cycling, generator ramping) while respecting power limits and reserve requirements.

### Prior Work

**1. Classical control approaches**:
- Automatic Generation Control (AGC) with PI controllers [Kundur, 1994]: Industry standard but cannot handle nonlinear dynamics or optimize multi-step costs
- Model Predictive Control (MPC) [Mohamed et al., 2012; Venkat et al., 2008]: Effective but requires accurate system models (costly to obtain, degrade with topology changes)

**2. Single-agent RL for power systems**:
- DQN/DDPG for generator dispatch [Zhang et al., 2020; Cao et al., 2020]: Proof-of-concept on small test systems (IEEE 14-bus), but centralized architecture doesn't scale
- Safe RL with constraints [Dalal et al., 2018 applied to power]: Barrier functions and Lagrangian methods, but limited to single controller

**3. Multi-agent RL foundations**:
- Independent learners (IQL) [Tan, 1993]: Each agent learns separately; suffers from non-stationary environment
- Centralized training with decentralized execution (CTDE) [Lowe et al., 2017; Foerster et al., 2018]: MADDPG, QMIX enable coordination via centralized critic during training
- Communication protocols [Sukhbaatar et al., 2016; Jiang & Lu, 2018]: CommNet, TarMAC allow explicit message passing between agents

**4. Power systems + MARL (limited prior work)**:
- [Zhou et al., 2020]: MADDPG for voltage control in distribution networks‚Äîdifferent problem (voltage vs. frequency)
- [Huang et al., 2021]: Multi-agent dispatch for economic optimization, but ignores frequency dynamics and safety constraints
- [Liu et al., 2023]: Independent Q-learning for AGC, but no comparison to coordinated methods or constraint handling

### Gap We Address

**No prior work systematically evaluates modern MARL algorithms (CTDE vs. communication-based vs. independent) on realistic frequency regulation with explicit safety constraints and renewable integration.** Specifically:

1. Compare coordination mechanisms: CTDE (MADDPG, QMIX) vs. learned communication (TarMAC) vs. independent learners
2. Develop constraint-aware training using safety layers [Dalal et al., 2018] adapted to multi-agent setting
3. Evaluate generalization to topology changes (N-1 contingencies) and distribution shift (weather patterns)
4. Use realistic power system simulator (PYPOWER/Pandapower) with validated models, not toy grids

---

## 3. Problem Definition

### Agent and Environment

- **Agents**: N = 20 controllable units in a transmission grid:
  - 5 battery energy storage systems (BESS)
  - 8 natural gas peaker plants (fast-response generators)
  - 7 demand response aggregators (controllable loads)
- **Environment**: IEEE 68-bus transmission system with stochastic renewable generation and load

### Formal Multi-Agent MDP Model

We define a cooperative MA-MDP: **M = (S, {A^i}, P, {R^i}, Œ≥, N)** with shared reward (team objective).

**Global state space S ‚äÜ ‚Ñù^m** (continuous, partially observable):
- Grid frequency at each bus: f_k ‚àà [59.5, 60.5] Hz for k = 1...68
- Active power output of all generators: P_g^j ‚àà [P_min^j, P_max^j] MW
- Renewable generation: P_solar ‚àà [0, 500] MW, P_wind ‚àà [0, 800] MW
- Aggregate load: P_load ‚àà [2000, 5000] MW
- Time of day: t_hour ‚àà [0, 24), t_day ‚àà [0, 7) (sin/cos encoded)
- Line flows: F_l ‚àà [-F_max^l, F_max^l] MW for critical transmission lines

**Local observation for agent i, O^i(s) ‚äÜ ‚Ñù^{d_i}**:
- Local bus frequency f_local
- Own power output P^i, available capacity margin (P_max^i - P^i)
- Renewable forecast (next 4 steps, 1-minute resolution)
- Aggregate system frequency deviation (broadcast signal): Œîf_sys = 1/68 ‚àë_k (f_k - 60)
- Messages from neighboring agents (if communication enabled): m_j for j ‚àà N(i)

**Action space for agent i, A^i ‚äÜ ‚Ñù**:
- Change in power output: Œî P^i ‚àà [-ŒîP_max^i, ŒîP_max^i] MW/min
- Constraints:
  - **Capacity limits**: P^i + Œî P^i ‚àà [P_min^i, P_max^i]
  - **Ramp rate limits**: |Œî P^i| ‚â§ R_max^i (batteries: 50 MW/min, gas: 10 MW/min, DR: 5 MW/min)
  - **Battery energy limits**: State of charge ‚àà [20%, 90%]

**Joint action**: a = (a^1, ..., a^N) ‚àà A = A^1 √ó ... √ó A^N

**Transition dynamics P(s' | s, a)**: Governed by power flow equations + swing dynamics:

1. **Swing equation** (frequency dynamics):
   ```
   2H ¬∑ df_k/dt = P_gen,k - P_load,k - ‚àë_l D_kl(f_k - f_l) / X_l
   ```
   where D_kl is damping, X_l is line reactance

2. **Power flow**: DC approximation for line flows (validated linearization for small frequency deviations)

3. **Stochastic disturbances**:
   - Load: Ornstein-Uhlenbeck process + hourly trend
   - Solar: Forecast + prediction error (œÉ = 15% of forecast)
   - Wind: Autoregressive model with weather-dependent variance

4. **Contingency events**: Random generator/line outages (N-1) with probability 0.001 per timestep

Dynamics are **partially unknown**: agents don't have access to full grid topology, line impedances, or precise load models. They must learn from interactions.

**Shared reward function R(s, a)** (team objective):
```
R(s, a) = -Œª_freq ¬∑ ‚àë_k (f_k - 60)¬≤ 
          - Œª_cost ¬∑ ‚àë_i C_i(P^i) ¬∑ |Œî P^i|
          - Œª_wear ¬∑ ‚àë_i W_i(|Œî P^i|)
          - Œª_violation ¬∑ ùüô[constraints violated]
```

**Terms**:
- **Frequency deviation penalty**: Œª_freq = 1000 (quadratic to heavily penalize large deviations)
- **Regulation cost**: C_i(P) = marginal cost of power adjustment (batteries: $5/MWh, gas: $50/MWh, DR: $30/MWh)
- **Wear cost**: W_i = degradation from cycling (batteries: $0.01/MW for state-of-health, gas: thermal stress)
- **Constraint violation**: Large penalty (10,000) if frequency exceeds ¬±0.5 Hz or power limits violated

**Discount factor Œ≥ = 0.95** (20 steps ‚âà 20 minutes planning horizon at 1-min control intervals)

### Assumptions

1. **Cooperative agents**: All agents share the team objective (no adversarial behavior or misaligned incentives)
2. **Partial observability**: Agents observe local state + limited broadcast info; cannot see full network state
3. **Communication topology**: Agents can exchange messages with neighbors in electrical topology (graph structure), with 2-second delay
4. **Unknown dynamics**: Grid model (admittance matrix, inertia constants) not provided; must be learned
5. **Stochastic environment**: Renewable generation and load are random but satisfy historical distributions
6. **Safety-critical constraints**: Hard constraints on frequency bounds; soft constraints on power limits (penalized but recoverable)

### Available Data

**Offline historical data** (for pre-training/validation):
- 6 months of 1-minute resolution SCADA data from ERCOT (Texas grid)
- Features: bus frequencies, generator outputs, load, renewable generation, prices
- Behavior policy: Historical AGC PI controllers + manual dispatch
- ~260,000 timesteps (limited compared to state-action space)

**Simulation environment**:
- **Power flow simulator**: Pandapower (Python) with IEEE 68-bus test system
- **Dynamics simulator**: Custom swing equation integrator (Runge-Kutta 4th order)
- **Renewable/load models**: Trained on historical data (NREL solar/wind profiles, ERCOT load)
- **Contingency scenarios**: Library of 50 N-1 outage events (generator and line failures)

**Software infrastructure**:
- **MARL framework**: PyMARL2, EPyMARL (supports CTDE algorithms)
- **RL libraries**: Stable-Baselines3 (for single-agent baselines), RLlib (distributed training)
- **Deep learning**: PyTorch for custom network architectures
- **Power system tools**: Pandapower, PYPOWER for steady-state validation
- **Compute**: Multi-GPU training (4x NVIDIA A100) for parallel environment rollouts

---

## 4. Proposed Method and Goals

### Candidate Methods

We will implement and compare four multi-agent approaches:

#### 4.1 Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [Primary CTDE Method]

**Rationale**: State-of-the-art CTDE algorithm that addresses non-stationarity through centralized critic during training, decentralized actor during execution.

**Architecture**:
- **Decentralized actors**: œÄ^i(a^i | o^i; Œ∏^i) for each agent i (only uses local observation)
- **Centralized critic**: Q(s, a^1, ..., a^N; œÜ) evaluates joint action given global state (available during training)
- **Training**: Each actor optimizes using policy gradient with centralized Q:
  ```
  ‚àá_Œ∏^i J = E[‚àá_Œ∏^i œÄ^i(o^i) ¬∑ ‚àá_a^i Q(s, a^1, ..., a^N)|_{a^i = œÄ^i(o^i)}]
  ```
- **Execution**: Only actors deployed (decentralized, no communication needed)

**Network design**:
- Actor: 3-layer MLP (256-128-64), tanh activation, outputs continuous action
- Critic: 4-layer MLP (512-256-128-64), takes concatenated state and all actions
- Target networks with soft updates (œÑ = 0.01)

**Constraint handling**: Project actions onto feasible set (ramp rate, capacity limits) using quadratic programming layer

#### 4.2 QMIX with Continuous Actions [Alternative CTDE Method]

**Rationale**: Value-based CTDE that learns factorized Q-function Q_tot = g(Q^1, ..., Q^N) where g is a monotonic mixing network. Often more sample-efficient than policy gradients.

**Architecture**:
- **Local Q-networks**: Q^i(o^i, a^i; Œ∏^i) for discrete action spaces
- **Modification for continuous actions**: Use NAF (Normalized Advantage Functions) or discretize action space into 21 bins
- **Mixing network**: Hypernetwork generates weights based on global state to combine local Q-values

**Advantage**: Enables centralized training with decentralized execution without explicit policy, potentially more stable

#### 4.3 Targeted Multi-Agent Communication (TarMAC) [Communication-Based Method]

**Rationale**: Learn when and what to communicate. Agents exchange learned feature representations to improve coordination without requiring full state access.

**Architecture**:
- **Communication protocol**: 
  ```
  m^i_t = signature_net^i(o^i_t, h^i_{t-1})  // what to send
  Œ±^i_j = attention(h^i, m^j)                 // who to listen to
  h^i_t = GRU(o^i_t, ‚àë_j Œ±^i_j m^j)          // message aggregation
  a^i_t = actor(h^i_t)                        // action selection
  ```
- **Signature network**: Learns compact message representation (dim=32)
- **Attention mechanism**: Decides communication importance (scaled dot-product)

**Training**: End-to-end with policy gradient; communication naturally learned to maximize return

**Advantage**: More realistic for deployment (only requires neighbor communication, not full state access)

#### 4.4 Independent Deep Deterministic Policy Gradient (IDDPG) [Baseline]

**Rationale**: Each agent learns independently treating others as part of environment. Provides lower bound on coordination benefit.

**Architecture**: Standard DDPG per agent with only local observation, no shared critic or communication

**Expected weakness**: Non-stationarity from other agents updating policies, likely suboptimal coordination

### Justification of Method Choices

- **MADDPG**: Proven effective for continuous control with multiple agents; centralized critic stabilizes training
- **QMIX**: Tests whether value factorization outperforms direct policy optimization in this domain
- **TarMAC**: Evaluates learned communication vs. fixed CTDE; more practical for realistic deployment with communication constraints
- **IDDPG**: Essential baseline to quantify coordination gains

All methods will incorporate **safety layers** [Dalal et al., 2018]:
- **Projection layer**: Projects policy output onto constraint set (capacity, ramp rates) via analytical solution
- **Safety critic**: Separate Q-network trained to predict constraint violation probability
- **Lagrangian relaxation**: For soft constraints, learn dual variables Œª to penalize violations:
  ```
  L_safe = L_RL + ‚àë_c Œª_c ¬∑ max(0, constraint_violation_c)
  ```

### Concrete Goals and Success Criteria

**Primary Objectives**:

1. **Frequency stability**: Maintain |f_k - 60 Hz| < 0.2 Hz for 99% of time (vs. 95% with baseline AGC)
2. **Cost reduction**: Achieve ‚â•25% reduction in regulation costs compared to PI-based AGC
3. **Coordination efficiency**: MADDPG/QMIX outperform IDDPG by ‚â•15% in cumulative reward
4. **Constraint satisfaction**: Zero critical violations (f outside [59.5, 60.5] Hz); <2% soft constraint violations (power limits)

**Performance Metrics**:

1. **Frequency regulation quality**:
   - Area Control Error (ACE): Integral of frequency deviation over episode
   - RoCoF: Maximum rate of change of frequency during disturbances
   - Nadir frequency: Minimum frequency after contingency events

2. **Economic metrics**:
   - Total regulation cost: ‚àë_t ‚àë_i C_i(P^i_t) ¬∑ |Œî P^i_t|
   - Dispatch efficiency: Ratio of renewable energy utilized vs. curtailed

3. **Safety metrics**:
   - Constraint violation rate: % timesteps with violations
   - Severity of worst violation: max_t |f_t - 60|

4. **Learning metrics**:
   - Sample efficiency: Reward vs. environment steps
   - Convergence stability: Variance of returns in final 100 episodes

5. **Coordination metrics** (for MARL):
   - Communication overhead: Bits transmitted per agent per timestep (TarMAC)
   - Credit assignment: Individual agent contribution via Shapley values

**Success Criteria**:

‚úÖ **Tier 1 (Minimum viable)**: MADDPG achieves frequency stability goal (99% within ¬±0.2 Hz) and 15% cost reduction

‚úÖ **Tier 2 (Expected)**: MADDPG/QMIX outperform IDDPG by ‚â•15% and handle N-1 contingencies without blackouts

‚úÖ **Tier 3 (Stretch)**: TarMAC matches MADDPG performance with <20% communication bandwidth; policy generalizes to unseen contingency types

### Evaluation Plan

**Baselines**:

1. **PI-based AGC** (industry standard): Proportional-integral controller with droop control
2. **Centralized MPC** (upper bound): Model predictive control with perfect model (oracle)
3. **IDDPG** (independent learners): Quantifies benefit of coordination
4. **Behavioral cloning**: Supervised learning on historical ERCOT data (offline benchmark)

**Training Protocol**:

1. **Pre-training phase** (optional):
   - Warm-start MADDPG with behavioral cloning on offline data (6 months)
   - Initialize dynamics model for MPC baseline via system identification

2. **Online training**:
   - Parallel environment rollouts (32 environments, 4 GPUs)
   - Episode length: 1440 steps (24 hours of operation)
   - Total training: 5M timesteps (‚âà3,500 episodes)
   - Replay buffer: 500k transitions
   - Exploration: Ornstein-Uhlenbeck noise on actions (œÉ decreases linearly)

3. **Curriculum learning**:
   - Stage 1 (0-1M steps): Normal operation, no contingencies
   - Stage 2 (1-3M steps): Introduce N-1 outages (5% of episodes)
   - Stage 3 (3-5M steps): Extreme scenarios (high renewable variability, multiple outages)

**Evaluation Scenarios**:

1. **Normal operation**: 100 episodes with historical weather/load patterns (different months than training)
2. **Contingency stress test**: 50 episodes with N-1 generator/line outages
3. **Renewable ramp events**: 30 episodes with extreme solar/wind forecast errors (3œÉ events)
4. **Topology changes**: 20 episodes with 10% fewer controllable generators (test graceful degradation)
5. **Distribution shift**: Evaluate on different season (summer vs. winter), different grid size (scale to IEEE 118-bus)

**Metrics Collection**:

- **Per-episode**: Total reward, ACE, constraint violations, cost breakdown
- **Per-timestep**: Frequency at all buses, line flows, agent actions
- **Statistical testing**: 10 random seeds, report mean ¬± 95% confidence interval
- **Visualization**: Frequency heatmaps, agent action timeseries, communication patterns (TarMAC)

**Ablation Studies**:

1. **Coordination mechanism**: MADDPG vs. QMIX vs. TarMAC vs. IDDPG
2. **Observation space**: Full state (cheating) vs. local + broadcast vs. local only
3. **Communication topology** (TarMAC): Electrical neighbors vs. k-hop neighbors vs. full connectivity
4. **Safety mechanisms**: With vs. without safety critic and projection layers
5. **Reward shaping**: Vary Œª_freq, Œª_cost, Œª_wear to understand tradeoffs

**Computational Requirements**:

- **Training**: ~200 GPU-hours per algorithm (4 GPUs √ó 50 hours)
- **Evaluation**: ~10 CPU-hours per method
- **Total**: ~1000 GPU-hours for all methods + ablations (feasible with institutional cluster)

### Feasibility and Risk Mitigation

‚úÖ **Highly Feasible**:
- Pandapower is mature, well-documented power system simulator
- PyMARL2/EPyMARL provide tested MARL implementations
- IEEE test cases widely available and validated
- Team has access to ERCOT historical data and compute cluster

‚ö†Ô∏è **Risk 1: Training instability in multi-agent setting**
- **Mitigation**: 
  - Use gradient clipping (norm=0.5)
  - Target network soft updates to reduce non-stationarity
  - If instability persists, fall back to sequential training (update one agent at a time)

‚ö†Ô∏è **Risk 2: Sim-to-real gap (if validating on real grid data)**
- **Mitigation**: 
  - Validate simulator against historical ERCOT data (frequency response to known events)
  - Add noise to simulator (¬±5% parameter uncertainty) for robustness
  - Domain randomization: Train on distribution of grid topologies

‚ö†Ô∏è **Risk 3: Insufficient exploration in safety-critical setting**
- **Mitigation**: 
  - Use importance sampling to oversample rare critical states in replay buffer
  - Safe exploration via optimistic constraint estimation [Berkenkamp et al., 2017]
  - If exploration too dangerous, leverage offline pre-training more heavily

‚ö†Ô∏è **Risk 4: Communication protocol learning may not converge (TarMAC)**
- **Mitigation**: 
  - Pre-train signature network with autoencoder objective (reconstruct observation from message)
  - Regularize communication (entropy bonus to encourage information sharing)
  - If fails, compare to hand-designed communication (broadcast frequency error)

‚ö†Ô∏è **Risk 5: Scalability to larger grids (118+ buses)**
- **Mitigation**: 
  - Use graph neural networks (GNN) for agent policies to exploit grid topology structure
  - Hierarchical control: Train area-level coordinators + local agents
  - Focus on 68-bus for main results; 118-bus as stretch goal

### Timeline (12 weeks)

**Weeks 1-2**: Environment setup and baselines
- Implement Pandapower simulation wrapper with swing dynamics
- Code PI-AGC and MPC baselines
- Validate simulator against ERCOT historical frequency response

**Weeks 3-4**: Single-agent methods
- Implement IDDPG baseline (independent learners)
- Debug training loop, replay buffer, safety projections
- Initial results on small 14-bus test system

**Weeks 5-7**: Multi-agent methods
- Implement MADDPG and QMIX
- Scale to 68-bus system
- Hyperparameter tuning (learning rates, replay buffer size, target update frequency)

**Week 8**: Communication-based method
- Implement TarMAC with attention mechanism
- Experiment with communication topologies

**Weeks 9-10**: Evaluation and ablations
- Run full evaluation suite (100+ episodes per method)
- Ablation studies (observation space, safety mechanisms, reward weights)
- Statistical significance testing

**Week 11**: Analysis and visualization
- Generate plots (frequency trajectories, cost breakdown, learning curves)
- Analyze failure modes (what causes constraint violations?)
- Contingency analysis (N-1 response quality)

**Week 12**: Report writing and presentation
- Draft final report with all results
- Create presentation slides
- Prepare demo video (visualizing learned policies)

---

## References

**Power systems fundamentals**:
- Kundur, P. (1994). *Power System Stability and Control*. McGraw-Hill.
- NERC (2023). "Frequency Response Initiative Report." North American Electric Reliability Corporation.

**Control and optimization**:
- Mohamed, T. H., et al. (2012). "Decentralized model predictive based load frequency control in an interconnected power system." *Energy Conversion and Management*.
- Venkat, A. N., et al. (2008). "Distributed MPC strategies for automatic generation control." *IEEE Transactions on Control Systems Technology*.

**Multi-agent reinforcement learning**:
- Tan, M. (1993). "Multi-agent reinforcement learning: Independent vs. cooperative agents." *ICML*.
- Lowe, R., et al. (2017). "Multi-agent actor-critic for mixed cooperative-competitive environments." *NeurIPS*.
- Foerster, J., et al. (2018). "Counterfactual multi-agent policy gradients." *AAAI*.
- Rashid, T., et al. (2018). "QMIX: Monotonic value function factorisation for decentralized multi-agent reinforcement learning." *ICML*.
- Sukhbaatar, S., et al. (2016). "Learning multiagent communication with backpropagation." *NeurIPS*.
- Jiang, J., & Lu, Z. (2018). "Learning attentional communication for multi-agent cooperation." *NeurIPS*.

**Safe reinforcement learning**:
- Dalal, G., et al. (2018). "Safe exploration in continuous action spaces." *arXiv:1801.08757*.
- Berkenkamp, F., et al. (2017). "Safe model-based reinforcement learning with stability guarantees." *NeurIPS*.

**Power systems + RL/MARL**:
- Zhang, Y., et al. (2020). "Deep reinforcement learning based volt-VAR optimization in smart distribution systems." *IEEE Transactions on Smart Grid*.
- Cao, D., et al. (2020). "Reinforcement learning and its applications in modern power and energy systems: A review." *Journal of Modern Power Systems and Clean Energy*.
- Zhou, S., et al. (2020). "Multi-agent reinforcement learning for coordinated voltage control." *IEEE PES General Meeting*.
- Huang, Q., et al. (2021). "Multi-agent deep reinforcement learning for HVAC control in commercial buildings." *IEEE Transactions on Smart Grid*.
- Liu, Y., et al. (2023). "Distributed reinforcement learning for decentralized automatic generation control." *Electric Power Systems Research*.
- Venkat, D., et al. (2022). "Economic and reliability impacts of RL-based frequency regulation." *IEEE Transactions on Power Systems*.

**Data sources**:
- U.S. Energy Information Administration (EIA). (2024). "Electric Power Monthly."
- ERCOT (Electric Reliability Council of Texas). SCADA historical data.
- NREL (National Renewable Energy Laboratory). Solar and wind generation profiles.
