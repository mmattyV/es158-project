%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
\usepackage{amsfonts, amsmath, amsthm}
\usepackage[letterpaper, total={6in, 9in}]{geometry}
\usepackage[noend]{algorithmic}
\usepackage[vlined,ruled,linesnumbered]{algorithm2e}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\title{Multi-Agent Reinforcement Learning for Real-Time Frequency Regulation in Power Grids}
\author{Your Name}
\date{\today}

\begin{document}
\maketitle

\input{sections/abstract.tex}
\input{sections/introduction.tex}
\input{sections/related-work.tex}
\input{sections/formulation.tex}
\input{sections/method.tex}
\input{sections/experiments.tex}
\input{sections/conclusion.tex}

\clearpage
\begin{center}
    {\Large\bf Appendix}
\end{center}
\appendix
\input{sections/app-proof-convergence.tex}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/abstract.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Renewable energy integration is disrupting power grid frequency control, creating faster dynamics and higher regulation costs. This project investigates multi-agent reinforcement learning for coordinating distributed controllable resources (batteries, generators, demand response) to maintain grid frequency at 60 Hz. We formulate the problem as a cooperative multi-agent MDP with continuous spaces, partial observability, and safety constraints. We compare three coordination approaches: centralized training with decentralized execution (MADDPG), value factorization (QMIX), and independent learning (IDDPG). All methods incorporate safety layers for constraint satisfaction. Evaluation on an IEEE 68-bus system aims to demonstrate 25\% cost reduction while maintaining frequency within $\pm$0.2 Hz for 99\% of operation.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/introduction.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

\subsection{Motivation}
Power grids must maintain frequency at 60 Hz by balancing generation and load. Renewable energy (solar/wind) now comprises 30\%+ of generation but provides no mechanical inertia, causing faster frequency dynamics and contributing to recent blackouts \cite{nerc2023}. Grid operators spend \$10B+ annually on frequency regulation. We need adaptive, data-driven control strategies.

\subsection{Problem Statement}
We consider a transmission grid with $N=20$ controllable units (batteries, gas generators, demand response) that must collectively stabilize frequency despite:
\begin{itemize}
    \item Stochastic renewable generation
    \item Time-varying loads
    \item Equipment outages (N-1 contingencies)
    \item Partial observability and communication delays
\end{itemize}

This is a \textbf{sequential decision making problem} where agents must coordinate to balance frequency stability, cost, and safety constraints.

\subsection{Why Multi-Agent RL?}
Traditional PI controllers cannot optimize multi-step costs. Model predictive control requires accurate models that degrade over time. Centralized RL faces scalability issues (exponential action space) and single points of failure. Multi-agent RL enables distributed control but introduces coordination challenges: non-stationarity, credit assignment, and emergent cooperation.

\subsection{Contributions}
\begin{enumerate}
    \item Detailed multi-agent MDP formulation with safety constraints
    \item Comparison of coordination mechanisms (CTDE vs. independent learning)
    \item Safety layers adapted to multi-agent settings
    \item Evaluation on realistic power system with validated dynamics
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/related-work.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related}

\subsection{Power System Control}
Traditional automatic generation control (AGC) uses PI controllers with fixed gains \cite{kundur1994}. Model predictive control improves performance but requires accurate system models that are expensive to maintain \cite{mohamed2012,venkat2008}.

\subsection{RL for Power Systems}
Deep RL has been applied to voltage control \cite{zhang2020} and economic dispatch \cite{cao2020}, but mostly assumes centralized control with full observability. Few works address frequency regulation specifically, and none systematically compare multi-agent coordination mechanisms with safety guarantees.

\subsection{Safe RL}
Safety-critical control requires constraint satisfaction. Approaches include constrained MDPs \cite{achiam2017}, safety layers that project actions onto safe sets \cite{dalal2018}, and shielding with formal guarantees \cite{alshiekh2018}.

\subsection{Multi-Agent RL}
Independent learners suffer from non-stationarity \cite{tan1993}. Centralized training with decentralized execution (CTDE) stabilizes learning: MADDPG uses centralized critics \cite{lowe2017}, while QMIX uses value factorization \cite{rashid2018}.

\subsection{Gap We Address}
No prior work systematically compares modern MARL algorithms on realistic frequency regulation with explicit safety constraints, validated dynamics, and evaluation on contingencies. Our work fills this gap.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/formulation.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation}
\label{sec:formulation}

\subsection{Multi-Agent MDP}
We define a cooperative MA-MDP:
\begin{equation}
\mathcal{M} = \langle \mathcal{N}, \mathcal{S}, \{\mathcal{A}^i\}_{i=1}^N, P, R, \gamma \rangle
\end{equation}

\textbf{Agents}: $\mathcal{N} = \{1, \ldots, 20\}$ controllable units (5 batteries, 8 gas plants, 7 demand response)

\textbf{Global state} $s_t \in \mathcal{S} \subseteq \mathbb{R}^{140}$:
\begin{itemize}
    \item Frequency at 68 buses: $\mathbf{f}_t \in [59.5, 60.5]^{68}$ Hz
    \item Generator outputs: $\mathbf{P}_t \in \mathbb{R}^{20}$ MW
    \item Renewable generation: $P_t^{\text{solar}}, P_t^{\text{wind}}$
    \item Load: $P_t^{\text{load}} \in [2000, 5000]$ MW
    \item Time features (hour, day)
\end{itemize}

\textbf{Local observation} $o_t^i \in \mathcal{O}^i \subseteq \mathbb{R}^{15}$ (partial observability):
\begin{itemize}
    \item Local frequency $f_{\text{local}}^i$
    \item Own output $P^i$ and capacity $(P_{\max}^i - P^i)$
    \item System frequency deviation $\Delta f_{\text{sys}} = \frac{1}{68}\sum_k (f_k - 60)$ (broadcast)
    \item Renewable forecast (4 steps ahead)
\end{itemize}

\textbf{Action} $a_t^i = \Delta P^i(t) \in [-\Delta P_{\max}^i, \Delta P_{\max}^i]$ MW/min

\textbf{Constraints}:
\begin{align}
P_{\min}^i &\leq P^i + \Delta P^i \leq P_{\max}^i \quad \text{(capacity)} \\
|\Delta P^i| &\leq R_{\max}^i \quad \text{(ramp rate)}
\end{align}

\textbf{Dynamics}: Governed by swing equation
\begin{equation}
2H \frac{df_k}{dt} = P_{\text{gen},k} - P_{\text{load},k} - \sum_{l} \frac{D_{kl}(f_k - f_l)}{X_l}
\end{equation}
plus stochastic disturbances (load, renewable forecast errors, N-1 outages). Dynamics are \textbf{unknown} to agents.

\textbf{Shared reward}:
\begin{equation}
R(s, \mathbf{a}) = -\lambda_{\text{freq}} \sum_k (f_k - 60)^2 - \lambda_{\text{cost}} \sum_i C_i |\Delta P^i| - \lambda_{\text{wear}} \sum_i W_i(|\Delta P^i|)
\end{equation}
with $\lambda_{\text{freq}} = 1000$, $\lambda_{\text{cost}} = 1$, $\lambda_{\text{wear}} = 0.1$, and large penalty for violations $|f_k - 60| > 0.5$ Hz.

\textbf{Objective}: Find policies $\{\pi^i\}_{i=1}^N$ that maximize
\begin{equation}
J = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R(s_t, \mathbf{a}_t)\right]
\end{equation}
subject to safety: $\Pr[|f_k - 60| > 0.5] < 0.01$ for all $k$.

Discount: $\gamma = 0.95$ (20-minute horizon).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/method.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Methods}
\label{sec:method}

\subsection{Multi-Agent DDPG (MADDPG)}
MADDPG \cite{lowe2017} uses centralized training with decentralized execution.

\textbf{Decentralized actors}: Each agent has policy $\pi^i(o^i; \theta^i): \mathcal{O}^i \to \mathcal{A}^i$

\textbf{Centralized critics}: $Q^i(s, a^1, \ldots, a^N; \phi^i)$ uses global state and all actions during training

\textbf{Training}: 
\begin{itemize}
    \item Critic minimizes TD error: $\mathcal{L}(\phi^i) = \mathbb{E}[(Q^i(s, \mathbf{a}) - (R + \gamma Q^i(s', \mathbf{a}')))^2]$
    \item Actor maximizes Q via gradient: $\nabla_{\theta^i} J = \mathbb{E}[\nabla_{\theta^i} \pi^i \nabla_{a^i} Q^i|_{a^i=\pi^i}]$
\end{itemize}

\textbf{Execution}: Only actors deployed using local observations (no communication needed)

\subsection{QMIX}
QMIX \cite{rashid2018} learns factorized Q-functions. We adapt to continuous actions via NAF:
\begin{equation}
Q^i(o^i, a^i) = V^i(o^i) - \frac{1}{2}(a^i - \mu^i(o^i))^2 P^i(o^i)
\end{equation}

Mixing network combines local Q-values monotonically:
\begin{equation}
Q_{\text{tot}}(s, \mathbf{a}) = g(Q^1, \ldots, Q^N, s)
\end{equation}
with $\partial Q_{\text{tot}}/\partial Q^i \geq 0$ ensuring individual-global-max property.

\subsection{Independent DDPG (IDDPG)}
Baseline where each agent learns independently. Expected to underperform due to non-stationarity.

\subsection{Safety Mechanisms}

\textbf{Action projection}: Project policy output onto constraint set
\begin{equation}
a^i = \arg\min_{a \in \mathcal{C}^i} \|\tilde{a}^i - a\|^2
\end{equation}
where $\mathcal{C}^i$ encodes capacity and ramp constraints.

\textbf{Safety critic}: Learn $Q_{\text{safe}}^i(o^i, a^i)$ predicting violation probability. Bias policy toward safe regions during execution.

\textbf{Lagrangian}: For soft constraints, learn dual variables:
\begin{equation}
\mathcal{L} = J(\theta) + \lambda(\mathbb{E}[\text{violation}] - \delta)
\end{equation}

All methods use these safety layers.

\subsection{Algorithm}
\begin{algorithm}[H]
\caption{MADDPG with Safety}
\For{episode $= 1, 2, \ldots$}{
    Reset environment, get observations $\{o_0^i\}$\;
    \For{$t = 0, 1, \ldots, T-1$}{
        Each agent: $\tilde{a}_t^i = \pi^i(o_t^i) + \text{noise}$\;
        Project: $a_t^i = \text{proj}_{\mathcal{C}^i}(\tilde{a}_t^i)$\;
        Execute $\mathbf{a}_t$, observe $R_t, s_{t+1}, \{o_{t+1}^i\}$\;
        Store transition in replay buffer\;
    }
    Sample minibatch, update critics and actors\;
    Soft update target networks\;
}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/experiments.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Environment}
\textbf{System}: IEEE 68-bus transmission network \\
\textbf{Simulator}: Pandapower + custom swing dynamics \\
\textbf{Data}: ERCOT historical data (6 months), NREL renewable profiles \\
\textbf{Episode}: 1440 steps (24 hours at 1-min resolution)

\subsection{Baselines}
\begin{itemize}
    \item \textbf{B1: PI-AGC}: Industry standard proportional-integral controller
    \item \textbf{B2: MPC}: Model predictive control (oracle with perfect model)
    \item \textbf{B3: Behavioral cloning}: Supervised learning on historical data
\end{itemize}

\subsection{Proposed Methods}
\begin{itemize}
    \item \textbf{M1: MADDPG}: Centralized training, decentralized execution
    \item \textbf{M2: QMIX}: Value factorization with NAF
    \item \textbf{M3: IDDPG}: Independent learners (baseline)
\end{itemize}

\subsection{Training}
\begin{itemize}
    \item 5M steps (≈3,500 episodes) across 32 parallel environments
    \item Learning rates: $\alpha_\pi = 10^{-4}$, $\alpha_Q = 10^{-3}$
    \item Batch size: 256, Replay buffer: 500K
    \item 10 random seeds, report mean $\pm$ 95\% CI
\end{itemize}

\subsection{Evaluation Scenarios}
\begin{itemize}
    \item \textbf{S1}: Normal operation (100 episodes)
    \item \textbf{S2}: N-1 contingencies (50 episodes) - generator/line outages
    \item \textbf{S3}: Renewable ramps (30 episodes) - extreme forecast errors
    \item \textbf{S4}: Distribution shift (50 episodes) - different season
\end{itemize}

\subsection{Metrics}
\textbf{Primary}:
\begin{itemize}
    \item Cumulative reward $\sum_t R_t$
    \item Frequency stability: \% time with $|f - 60| < 0.2$ Hz (target: >99\%)
    \item Regulation cost (total \$)
    \item Constraint violations: \% time $|f - 60| > 0.5$ Hz (target: <1\%)
\end{itemize}

\textbf{Secondary}: Area control error, max deviation, renewable utilization

\subsection{Ablations}
\begin{itemize}
    \item Coordination: MADDPG vs. QMIX vs. IDDPG
    \item Observations: Full state vs. local+broadcast vs. local only
    \item Safety: No safety vs. projection vs. critic vs. both
    \item Reward weights: Vary $\lambda_{\text{freq}}, \lambda_{\text{cost}}$
\end{itemize}

\subsection{Success Criteria}
\textbf{Tier 1 (minimum)}: MADDPG achieves >99\% frequency stability, >15\% cost reduction, zero critical failures

\textbf{Tier 2 (expected)}: MADDPG/QMIX outperform IDDPG by >15\%, handle N-1 contingencies, >25\% cost reduction

\textbf{Tier 3 (stretch)}: Achieve 90\% of MPC performance, generalize to larger grids

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/conclusion.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

This project provides the first systematic comparison of MARL algorithms for power grid frequency regulation with safety constraints. We expect to demonstrate that coordinated multi-agent RL (MADDPG/QMIX) can achieve significant cost savings (25\%+) over traditional control while maintaining reliability and handling contingencies.

\textbf{Future work}: Scale to larger grids (100+ buses), sim-to-real transfer, adversarial robustness, integration with electricity markets.

\textbf{Broader impacts}: Enabling higher renewable penetration reduces carbon emissions and costs. However, deployment requires careful consideration of cybersecurity, accountability, and equitable access.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% sections/app-proof-convergence.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hyperparameters and Implementation}

\subsection{Network Architectures}
\textbf{Actor}: 3-layer MLP [15, 256, 128, 64, 1] with ReLU, tanh output \\
\textbf{Critic}: 4-layer MLP [160, 512, 256, 128, 1] with ReLU

\subsection{Key Hyperparameters}
\begin{itemize}
    \item Discount: $\gamma = 0.95$
    \item Target update: $\tau = 0.01$
    \item Exploration: OU noise $\sigma = 0.2 \to 0.05$ (linear decay)
    \item Gradient clipping: norm = 0.5
\end{itemize}

\subsection{Computational Requirements}
Training: ≈200 GPU-hours per method on 4 NVIDIA A100s \\
Total: ≈1000 GPU-hours (feasible with institutional cluster)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% refs.bib (create this file)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sample references - replace with actual citations

@techreport{nerc2023,
  author = {NERC},
  title = {Frequency Response Initiative Report},
  institution = {North American Electric Reliability Corporation},
  year = {2023}
}

@book{kundur1994,
  author = {Kundur, P.},
  title = {Power System Stability and Control},
  publisher = {McGraw-Hill},
  year = {1994}
}

@article{mohamed2012,
  author = {Mohamed, T. H. and others},
  title = {Decentralized model predictive based load frequency control},
  journal = {Energy Conversion and Management},
  year = {2012}
}

@article{venkat2008,
  author = {Venkat, A. N. and others},
  title = {Distributed MPC strategies for automatic generation control},
  journal = {IEEE Transactions on Control Systems Technology},
  year = {2008}
}

@article{zhang2020,
  author = {Zhang, Y. and others},
  title = {Deep reinforcement learning based volt-VAR optimization},
  journal = {IEEE Transactions on Smart Grid},
  year = {2020}
}

@article{cao2020,
  author = {Cao, D. and others},
  title = {Reinforcement learning and its applications in power systems},
  journal = {Journal of Modern Power Systems and Clean Energy},
  year = {2020}
}

@inproceedings{achiam2017,
  author = {Achiam, J. and others},
  title = {Constrained Policy Optimization},
  booktitle = {ICML},
  year = {2017}
}

@article{dalal2018,
  author = {Dalal, G. and others},
  title = {Safe exploration in continuous action spaces},
  journal = {arXiv:1801.08757},
  year = {2018}
}

@inproceedings{alshiekh2018,
  author = {Alshiekh, M. and others},
  title = {Safe reinforcement learning via shielding},
  booktitle = {AAAI},
  year = {2018}
}

@inproceedings{tan1993,
  author = {Tan, M.},
  title = {Multi-agent reinforcement learning},
  booktitle = {ICML},
  year = {1993}
}

@inproceedings{lowe2017,
  author = {Lowe, R. and others},
  title = {Multi-agent actor-critic for mixed cooperative-competitive environments},
  booktitle = {NeurIPS},
  year = {2017}
}

@inproceedings{rashid2018,
  author = {Rashid, T. and others},
  title = {QMIX: Monotonic value function factorisation},
  booktitle = {ICML},
  year = {2018}
}
