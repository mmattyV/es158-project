\begin{abstract}
We implement Multi-Agent Proximal Policy Optimization (MAPPO) for real-time frequency regulation in a 68-bus power grid with 20 heterogeneous agents (5 batteries, 8 gas generators, 7 demand response units). Our centralized training, decentralized execution (CTDE) approach addresses the cooperative Multi-Agent MDP with continuous spaces, partial observability, and safety constraints. Training for 1000 episodes demonstrates learning convergence with MAPPO outperforming independent learners by 60\%. However, high reward variance and 30\% early termination rate indicate performance gaps. We identify four failure modes (insufficient response, miscoordination, oscillations, capacity saturation) and propose concrete improvements: reward shaping, enhanced observations, curriculum learning, and PI-AGC baseline comparison for the final report.
\end{abstract}
