\section{Training Challenges \& Solutions}
\label{sec:training}

\subsection{Initial Problems and Fixes}

Early training exhibited severe instability: critic loss exploding to $10^{13}$, episodes terminating at $\sim$50 steps, and rewards in millions (negative). We systematically debugged these issues:

\begin{center}
\begin{tabular}{p{3.5cm}p{5cm}}
\toprule
\textbf{Problem} & \textbf{Solution} \\
\midrule
Exploding critic loss & Scaled rewards by $\div 100,000$ \\
Immediate termination & Curriculum learning with relaxed bounds \\
Agents doing nothing & Added survival + stability bonuses \\
Capacity mismatch & Reduced load range to [1500, 3000] MW \\
Poor frequency signal & Normalized to frequency \textit{deviations} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Key Fix: Capacity Mismatch}

The most critical issue: with load range [2000, 5000] MW but only $\sim$3300 MW controllable generation, agents \textit{could not} balance the grid at high loads. Reducing to [1500, 3000] MW made learning feasible.

\subsection{Reward Function}

We redesigned rewards to align with curriculum bounds:
\begin{align}
R_t &= -2000 \textstyle\sum_k (f_k - 60)^2 - 1000 \textstyle\sum_k (\exp(2 \cdot \text{approach}_k) - 1) \notag \\
&\quad - \textstyle\sum_i C_i |a^i_t| - 0.05 \textstyle\sum_i W_i (a^i_t)^2 + 5000 \cdot n_{\text{stable}} + 5000
\end{align}
where $\text{approach}_k$ measures proximity to the \textit{current curriculum} termination bound, not fixed $\pm 0.5$ Hz.

\subsection{Lessons Learned}

\begin{enumerate}[nosep]
    \item \textbf{Ensure physical feasibility} before training---verify optimal policy can achieve the goal
    \item \textbf{Reward scaling is critical}---normalize to $[-10, +10]$ range
    \item \textbf{Align reward with termination}---if curriculum changes bounds, rewards must track
    \item \textbf{Normalize observations}---use deviations, not raw values
\end{enumerate}

These fixes transformed training from divergent to convergent. However, as Section~\ref{sec:experiments} shows, deeper multi-agent coordination challenges remain.
