\begin{abstract}
We implement Multi-Agent Proximal Policy Optimization (MAPPO) for real-time frequency regulation in a simulated 20-bus power grid with 10 heterogeneous agents (2 batteries, 5 gas generators, 3 demand response units). Our centralized training, decentralized execution (CTDE) approach addresses a cooperative Multi-Agent MDP with continuous state/action spaces, partial observability, and safety constraints. Through systematic debugging---addressing reward scaling, capacity mismatch, observation normalization, and curriculum learning---we achieved stable value function learning with critic loss converging from $\sim$29 to $\sim$22. However, policy performance exhibited the characteristic ``forgetting'' phenomenon of multi-agent systems: reward peaked at $\sim$165 around episode 1000 before degrading to $\sim$120 by episode 2000. We analyze this coordination collapse, attributing it to non-stationarity, credit assignment difficulties, and the exponential complexity of coordinating 10 independent agents under partial observability. Our results highlight both the promise and fundamental challenges of applying deep MARL to safety-critical infrastructure control.
\end{abstract}
