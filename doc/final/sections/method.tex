\section{Method: MAPPO}
\label{sec:method}

We selected MAPPO over MADDPG for stability (clipped objective), empirical performance on cooperative benchmarks~\cite{yu2021}, and hyperparameter robustness.

\subsection{Architecture}

\textbf{Actor} $\pi(a|o;\theta)$: [15 $\rightarrow$ 128 $\rightarrow$ 128 $\rightarrow$ 1] with LayerNorm/ReLU, outputs Gaussian $\mathcal{N}(\mu, \sigma^2)$, $\sim$19K parameters shared across agents.

\textbf{Critic} $V(s;\phi)$: [55 $\rightarrow$ 256 $\rightarrow$ 256 $\rightarrow$ 1] with LayerNorm/ReLU, takes global state, $\sim$81K parameters.

During training, critic accesses full state; during execution, actors use only local observations (CTDE).

\subsection{Training Algorithm}

Collect rollouts until buffer size $B=2048$. Compute advantages via GAE-$\lambda$:
\begin{equation}
A_t = \sum_{l=0}^{T-t} (\gamma\lambda)^l \delta_{t+l}, \quad \delta_t = R_t + \gamma V(s_{t+1}) - V(s_t)
\end{equation}

Update policy via PPO clipped objective over 10 epochs:
\begin{equation}
L^{\text{CLIP}} = \mathbb{E}\left[\min\left(r_t A_t, \text{clip}(r_t, 1\!-\!\epsilon, 1\!+\!\epsilon) A_t\right)\right] + \beta_{\text{ent}} H(\pi)
\end{equation}
where $r_t = \pi_\theta(a_t|o_t)/\pi_{\theta_{\text{old}}}(a_t|o_t)$. Update critic via MSE: $L^{\text{CRITIC}} = \mathbb{E}[(V(s_t) - G_t)^2]$.

\subsection{Hyperparameters}

\begin{center}
\begin{tabular}{lclc}
\toprule
Actor LR & $3 \times 10^{-4}$ & Critic LR & $1 \times 10^{-3}$ \\
GAE $\lambda$ & 0.95 & Discount $\gamma$ & 0.99 \\
Entropy coef & 0.02 & Value coef & 0.5 \\
Clip $\epsilon$ & 0.2 & Grad norm & 0.5 \\
Buffer & 2048 & Batch & 256 \\
\bottomrule
\end{tabular}
\end{center}

Implementation: $\sim$1500 lines across \texttt{power\_grid\_env.py} (775 lines), \texttt{networks.py}, \texttt{mappo.py}, \texttt{buffer.py}, \texttt{train.py} with TensorBoard logging.
