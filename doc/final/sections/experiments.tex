\section{Experiments \& Results}
\label{sec:experiments}

\subsection{Setup}

Training: 2000 episodes, max 500 steps, buffer 2048, batch 256, 10 PPO epochs. Environment: load [1500, 3000] MW, 7 renewable sources, N-1 contingencies ($p=0.001$), 2s SCADA delay, 4-stage curriculum.

\subsection{Training Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\linewidth]{final_graphs/episode_reward.png}
    \includegraphics[width=0.48\linewidth]{final_graphs/episode_length.png}
    \caption{\textbf{Left:} Episode reward peaks at $\sim$170 (ep 1000) then declines to $\sim$120---the ``forgetting'' phenomenon. \textbf{Right:} Episode length varies 350--400 steps with late-training degradation.}
    \label{fig:reward_length}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\linewidth]{final_graphs/critic_loss.png}
    \includegraphics[width=0.48\linewidth]{final_graphs/reward_wear.png}
    \caption{\textbf{Left:} Critic loss converges from $\sim$29 to $\sim$22 despite policy degradation. \textbf{Right:} Wear costs decrease from $\sim$0.007 to $\sim$0.003---agents learned smoother control.}
    \label{fig:loss_wear}
\end{figure}

Figure~\ref{fig:reward_length} shows three phases: (1) rapid learning (ep 0--500, reward $\sim$130$\rightarrow$150); (2) peak performance (ep 500--1200, reward $\sim$165); (3) degradation (ep 1200--2000, reward $\rightarrow$120). Critically, Figure~\ref{fig:loss_wear} shows critic loss \textit{continued decreasing} even as policy degraded---the value function learned accurately, but the actor couldn't exploit it.

\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Early} & \textbf{Peak} & \textbf{Late} \\
\midrule
Episode Reward & $\sim$140 & $\sim$165 & $\sim$120 \\
Episode Length & $\sim$375 & $\sim$390 & $\sim$360 \\
Critic Loss & $\sim$28 & $\sim$25 & $\sim$22 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{The Multi-Agent Coordination Challenge}

Our results illustrate a fundamental MARL difficulty: \textbf{training instability from non-stationarity}. Despite stable critic learning, policy performance exhibited ``forgetting.''

\textbf{Why MAPPO struggles with 10 agents:}
\begin{enumerate}[nosep]
    \item \textbf{Non-stationarity:} Each agent's environment changes as others update, violating MDP assumptions~\cite{tan1993}
    \item \textbf{Credit assignment:} Shared rewards provide no per-agent contribution signal~\cite{foerster2018}
    \item \textbf{Exponential complexity:} Joint policy space grows exponentially with agent count
    \item \textbf{Partial observability:} 15-dim local obs hides coordination information
    \item \textbf{Theoretical hardness:} Dec-POMDP optimal policies are NEXP-complete~\cite{bernstein2002}
\end{enumerate}

The decreasing critic loss during policy degradation is diagnostic: the value function correctly learned returns were declining, but the actor couldn't escape the coordination collapse.

\subsection{Positive Findings}

Despite challenges: (1) critic loss converged ($10^{13} \rightarrow 22$); (2) wear costs halved (smoother control); (3) episodes reached 350--400 steps (vs. $\sim$50 before fixes); (4) peak coordination achieved reward $\sim$165, proving 10-agent coordination \textit{is possible}.

\subsection{Limitations}

Simplified physics (linearized swing equation), reduced scale (20 buses vs. hundreds), no baseline comparison, and training instability limits practical deployment. Extensions like QMIX~\cite{rashid2020}, attention mechanisms, or hierarchical control could address coordination challenges.
