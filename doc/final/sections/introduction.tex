\section{Introduction}
\label{sec:introduction}

Modern power grids with $>30\%$ renewable penetration face frequency stability challenges due to reduced inertia and stochastic generation, causing \$10B+ annual regulation costs~\cite{nerc2023}. Traditional PI controllers and MPC struggle to scale with grid complexity~\cite{kundur1994,venkat2008}. Multi-agent reinforcement learning offers coordinated, adaptive control with potential 20--40\% cost reduction~\cite{venkat2022}.

We explore \textbf{Centralized Training, Decentralized Execution (CTDE)} using MAPPO, where a centralized critic observes global state during training, but agents execute using only local observations. We formulate frequency regulation as a cooperative MA-MDP with $N=10$ heterogeneous agents (2 batteries, 5 gas plants, 3 demand response units) across a 20-bus network governed by swing equation dynamics.

\textbf{Challenges:} Continuous spaces ($\mathcal{S} \subseteq \mathbb{R}^{55}$, $\mathcal{O}^i \in \mathbb{R}^{15}$), partial observability, stochastic disturbances, hard safety constraints ($\pm 1.5$ Hz), and multi-agent coordination (non-stationarity, credit assignment).

\textbf{Contributions:} (1) 20-bus power grid simulator with realistic dynamics; (2) MAPPO implementation with CTDE; (3) systematic debugging methodology for reward scaling, capacity matching, and curriculum learning; (4) analysis of multi-agent coordination challenges with experimental results.
